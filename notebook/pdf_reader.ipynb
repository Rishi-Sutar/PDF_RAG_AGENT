{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Richie\\anaconda3\\envs\\rag_app\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = PyPDFLoader(\"../azure.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'convincing me to write this title. The book improved in manifold ways through valuable comments from all the reviewers, time and again. Adrian Raposo did a commendable job helping develop the content as well as coordinating the overall project management. This book would not have been in its current shape had it not received the perfect touch of the technical editor, Abhishek Kotian, and also all the proofreaders.\\nSpecial thanks to my colleagues, Kamal and Mahananda. Kamal took time to get'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[10].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gemini_api_key = os.environ['google_api_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=gemini_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI( model=\"gemini-1.5-flash\",google_api_key=gemini_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = FAISS.from_documents(documents,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a PDF document expert specializing in extracting accurate answers from complex texts.\n",
    "Utilize the provided context to deliver precise and concise answers.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Provide a well-informed and detailed answer based on the context:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", template),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = create_retrieval_chain(retriever, chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What is  Evaluation metrics?', 'context': [Document(metadata={'source': '../azure.pdf', 'page': 120}, page_content='metrics that are defined. Often, one metric may not be sufficient to take a decision. To start with, you may look at accuracy, but at times it might be deceptive. Consider a case where you are making a prediction for a rare disease where in reality, 99 percent negative cases and 1 percent of positive cases appear. If your classification model predicts all the cases as true negatives, then the accuracy is still 99 percent. In this case, the F1 score might be useful as it would give you a clear'), Document(metadata={'source': '../azure.pdf', 'page': 116}, page_content=\"Consider a case where you need to predict the housing price not as a number, but as \\ncategories, such as greater than 100K or less than 100K. In this case, though you are predicting the housing price, you are indeed predicting a class or category for the \\nhousing price and hence, it's a classification problem.\\nYou build a classification model by training an algorithm with the given training \\ndata. In the training dataset, the class or target variable is already known.\\nEvaluation metrics\"), Document(metadata={'source': '../azure.pdf', 'page': 116}, page_content='data. In the training dataset, the class or target variable is already known.\\nEvaluation metrics\\nSuppose that you have built a model and trained a classification algorithm with the \\ndataset in Table 7.1 as the training data. Now, you are using the following table as \\nyour test data. As you can see, the last column has the predicted class.\\nAge Tumor size Actual class Predicted class\\n32 135 0 0 TN\\n47 121 0 1 FP\\n28 156 1 0 FN\\n45 162 1 1 TP\\n77 107 0 1 FP\\nTrue positive'), Document(metadata={'source': '../azure.pdf', 'page': 112}, page_content='Regression Models[ 88 ]The first row in the preceding screenshot shows the metrics for the model connected \\nto the first input of the Evaluate Model and in this case, it shows the Neural \\nNetwork Regression module and the second row for the second model: the Boosted')], 'answer': 'The provided text doesn\\'t explicitly define \"Evaluation metrics\" but it provides a context for understanding their importance in the context of building and assessing classification models. \\n\\nHere\\'s what we can infer about evaluation metrics based on the text:\\n\\n* **Evaluation metrics are used to assess the performance of a classification model.** They provide a way to quantify how well the model is able to predict the correct class (or category) for given data.\\n* **Different metrics focus on different aspects of model performance.** For example, accuracy might be misleading in cases with imbalanced data, while the F1 score offers a more robust measure in such scenarios.\\n* **The choice of evaluation metric depends on the specific problem and the desired outcome.** In the text, the example of predicting housing prices highlights how even a seemingly regression problem (predicting a numerical value) can be treated as a classification problem if the output is categorized (greater than 100K or less than 100K).\\n\\n**Essentially, evaluation metrics provide a way to objectively measure and compare the performance of different classification models, allowing you to choose the best model for your specific needs.** \\n\\nThe text also mentions some specific metrics like accuracy and F1 score, but it doesn\\'t provide a comprehensive list. To get a more complete understanding of evaluation metrics, you would need to refer to additional resources on classification model evaluation. \\n'}\n"
     ]
    }
   ],
   "source": [
    "# results = rag_chain.invoke({\"input\": \"What is  Evaluation metrics?\"})\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The confusion matrix is a table that helps visualize the performance of a classification model. It shows how well the model predicted the actual classes by comparing the predicted labels with the true labels. \n",
      "\n",
      "In the context provided, the confusion matrix is used to analyze the performance of a model that predicts the \"High\" class.  It shows the number of instances that were correctly and incorrectly classified as \"High\" compared to the actual class. \n",
      "\n",
      "For example, if a model predicts an instance as \"High\" but the actual class is \"Low,\" it's considered a **false positive**.  If a model predicts an instance as \"Low\" but the actual class is \"High,\" it's considered a **false negative**. \n",
      "\n",
      "Answer: The provided text focuses on the basics of using Azure Machine Learning Studio, but doesn't provide a specific example. However, I can give you a general example of how to use it for a simple machine learning task:\n",
      "\n",
      "**Scenario:** You want to predict if a customer will click on an ad based on their demographics and browsing history.\n",
      "\n",
      "**Steps:**\n",
      "\n",
      "1. **Create an Azure Machine Learning Workspace:**\n",
      "   - Go to https://studio.azureml.net/home\n",
      "   - Sign in with your Azure account.\n",
      "   - If you don't have a workspace, create one by clicking \"Create new workspace.\"\n",
      "   - Give your workspace a name and choose a resource group.\n",
      "\n",
      "2. **Import Data:**\n",
      "   - In ML Studio, you'll see the \"EXPERIMENTS\" tab on the left.\n",
      "   - Click \"New Experiment\" and give it a name.\n",
      "   - Drag and drop the \"Dataset\" module from the \"Data\" category onto the canvas.\n",
      "   - Choose the option to \"Upload Dataset\" and select your CSV file containing customer data (demographics, browsing history, and whether they clicked on the ad).\n",
      "\n",
      "3. **Prepare Data:**\n",
      "   - Add a \"Select Columns in Dataset\" module to select the relevant features (demographics, browsing history) for prediction.\n",
      "   - You might need to use other data transformation modules like \"Clean Missing Values\" or \"Normalize Data\" to prepare your data for training.\n",
      "\n",
      "4. **Train a Model:**\n",
      "   - Drag and drop a machine learning algorithm module from the \"Machine Learning\" category. For example, you can use \"Two-Class Logistic Regression\" or \"Decision Forest.\"\n",
      "   - Connect the prepared data to the input of the chosen algorithm.\n",
      "   - Set any necessary parameters for the algorithm.\n",
      "\n",
      "5. **Evaluate Model:**\n",
      "   - Add an \"Evaluate Model\" module to assess the performance of your trained model.\n",
      "   - You can use metrics like accuracy, precision, recall, and AUC to evaluate the model's effectiveness.\n",
      "\n",
      "6. **Deploy Model:**\n",
      "   - Once satisfied with the model's performance, you can deploy it as a web service for real-time predictions.\n",
      "   - Drag and drop a \"Deploy Web Service\" module.\n",
      "   - Connect the trained model to this module.\n",
      "   - Choose a deployment configuration and deploy the web service.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "- ML Studio has a drag-and-drop interface, making it easy to build and connect modules.\n",
      "- It offers various algorithms, data transformation modules, and visualization tools.\n",
      "- You can use R or Python code for more complex customization.\n",
      "- The \"EXPERIMENTS\" tab lets you manage, run, and save your experiments.\n",
      "- The \"WEB SERVICES\" tab allows you to publish trained models for real-time predictions.\n",
      "\n",
      "This is a simplified example, but it demonstrates the basic workflow of using Azure Machine Learning Studio. You can find detailed tutorials and documentation on the Microsoft Azure website for more specific use cases and advanced features. \n",
      "\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "# while True:\n",
    "#     user_question = input(\"Enter your question (or type 'exit' to quit): \")\n",
    "#     if user_question.lower() == 'exit':\n",
    "#         print(\"Exiting...\")\n",
    "#         break\n",
    "#     print(user_question)\n",
    "#     results = rag_chain.invoke({\"input\": user_question})\n",
    "#     print(\"Answer:\", results['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
