{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI  \n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = PyPDFLoader(\"../azure.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'convincing me to write this title. The book improved in manifold ways through valuable comments from all the reviewers, time and again. Adrian Raposo did a commendable job helping develop the content as well as coordinating the overall project management. This book would not have been in its current shape had it not received the perfect touch of the technical editor, Abhishek Kotian, and also all the proofreaders.\\nSpecial thanks to my colleagues, Kamal and Mahananda. Kamal took time to get'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[10].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gemini_api_key = os.environ['google_api_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=gemini_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI( model=\"gemini-1.5-flash\",google_api_key=gemini_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = FAISS.from_documents(documents,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance \n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question, \n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"what is Evaluation metric\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'evaluate model  86\\nevaluate recommender  135evaluation metrics, classification\\nabout  92\\naccuracy  93\\nArea Under the Curve (AUC)  95\\nF1 score  94false negative  93false positive  92\\nmatric  96\\nprecision  94\\nrecall  94receiver operating characteristics  \\n(ROC) graph  95\\nthreshold  94true negative  93true positive  92\\nExecute Python Script module  143-145Execute R Script module  149, 150experiment\\npreparing, to publish  156\\nF'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a PDF document expert specializing in extracting accurate answers from complex texts.\n",
    "Utilize the provided context to deliver precise and concise answers.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Provide a well-informed and detailed answer based on the context:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", template),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = create_retrieval_chain(retriever, chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What is  Evaluation metrics?', 'context': [Document(metadata={'source': '../azure.pdf', 'page': 120}, page_content='metrics that are defined. Often, one metric may not be sufficient to take a decision. To start with, you may look at accuracy, but at times it might be deceptive. Consider a case where you are making a prediction for a rare disease where in reality, 99 percent negative cases and 1 percent of positive cases appear. If your classification model predicts all the cases as true negatives, then the accuracy is still 99 percent. In this case, the F1 score might be useful as it would give you a clear'), Document(metadata={'source': '../azure.pdf', 'page': 116}, page_content=\"Consider a case where you need to predict the housing price not as a number, but as \\ncategories, such as greater than 100K or less than 100K. In this case, though you are predicting the housing price, you are indeed predicting a class or category for the \\nhousing price and hence, it's a classification problem.\\nYou build a classification model by training an algorithm with the given training \\ndata. In the training dataset, the class or target variable is already known.\\nEvaluation metrics\"), Document(metadata={'source': '../azure.pdf', 'page': 116}, page_content='data. In the training dataset, the class or target variable is already known.\\nEvaluation metrics\\nSuppose that you have built a model and trained a classification algorithm with the \\ndataset in Table 7.1 as the training data. Now, you are using the following table as \\nyour test data. As you can see, the last column has the predicted class.\\nAge Tumor size Actual class Predicted class\\n32 135 0 0 TN\\n47 121 0 1 FP\\n28 156 1 0 FN\\n45 162 1 1 TP\\n77 107 0 1 FP\\nTrue positive'), Document(metadata={'source': '../azure.pdf', 'page': 112}, page_content='Regression Models[ 88 ]The first row in the preceding screenshot shows the metrics for the model connected \\nto the first input of the Evaluate Model and in this case, it shows the Neural \\nNetwork Regression module and the second row for the second model: the Boosted')], 'answer': 'The provided context doesn\\'t explicitly define \"Evaluation metrics\" but it does explain their purpose and importance. Here\\'s what we can gather:\\n\\n**Evaluation metrics are used to assess the performance of a machine learning model, particularly in the context of classification problems.** They provide a way to quantify how well a model is able to predict the correct class for new data.\\n\\n**Here\\'s what the context highlights about evaluation metrics:**\\n\\n* **Multiple metrics are often needed:**  A single metric like accuracy might not be sufficient to understand a model\\'s performance. This is because accuracy can be misleading in cases with imbalanced datasets (like the example of predicting a rare disease). \\n* **Metrics like F1 score can be helpful:**  The context mentions the F1 score as an alternative metric that can provide a clearer picture of model performance when dealing with imbalanced datasets. \\n* **Metrics are used to compare models:** The context mentions comparing the performance of two models (Neural Network Regression and Boosted Decision Tree) using evaluation metrics.\\n\\n**In summary:** Evaluation metrics are crucial for understanding the effectiveness of classification models. They provide a way to objectively measure how well a model performs and help in choosing the best model for a specific task. \\n'}\n"
     ]
    }
   ],
   "source": [
    "results = rag_chain.invoke({\"input\": \"What is  Evaluation metrics?\"})\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is linear regression\n",
      "Answer: Linear regression is a regression algorithm used in ML Studio. It aims to fit a line to a dataset, making it a popular and historically significant regression method. \n",
      "\n",
      "ML studio pros and cons\n",
      "Answer: ## ML Studio: Pros and Cons\n",
      "\n",
      "**Pros:**\n",
      "\n",
      "* **Ease of Use:** ML Studio provides a visual, drag-and-drop interface, making it easy to build, test, and deploy predictive models without extensive coding knowledge.\n",
      "* **Comprehensive Platform:** It offers a complete platform for predictive analytics, encompassing development, testing, and deployment.\n",
      "* **Cloud-Based:** Being browser-based and hosted on Azure, ML Studio offers accessibility from any modern browser and eliminates the need for local software installations.\n",
      "* **Collaborative Environment:** ML Studio enables sharing your work with others, facilitating collaboration and knowledge sharing. \n",
      "\n",
      "**Cons:**\n",
      "\n",
      "* **Limited Flexibility:** While providing a user-friendly interface, ML Studio's out-of-the-box modules may not be sufficient for complex data preparation tasks, like applying wavelet transforms, or advanced data visualization requirements.\n",
      "* **Coding Necessity:** For real-world predictive analytics solutions, ML Studio's capabilities may be limited, requiring coding and integration to address specific needs. \n",
      "\n",
      "**In summary:** \n",
      "\n",
      "ML Studio is a great tool for beginners and those seeking a quick and intuitive way to build predictive models. However, for complex projects requiring advanced data manipulation or visualization, additional coding and integration with ML Studio may be necessary. \n",
      "\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_question = input(\"Enter your question (or type 'exit' to quit): \")\n",
    "    if user_question.lower() == 'exit':\n",
    "        print(\"Exiting...\")\n",
    "        break\n",
    "    print(user_question)\n",
    "    results = rag_chain.invoke({\"input\": user_question})\n",
    "    print(\"Answer:\", results['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
